{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><b>Capítulo 4</b></h1>\n",
    "<h1 align=center>Scraping usando <i>pyquery</i> - uma biblioteca Python</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pyquery* é uma biblioteca que usa a biblioteca *lxml* para interagir com o ambiente de uma forma mais fácil. Utiliza também os seletores CSS:\n",
    "\n",
    "`page.find('a').attr('href')`\n",
    "\n",
    "## Explorando o *pyquery*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fn', '__add__', '__call__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__html__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_append', '_copy', '_css_to_xpath', '_extend', '_filter_only', '_get_root', '_next_all', '_prev_all', '_translator_class', '_traverse', '_traverse_parent_topdown', 'addClass', 'add_class', 'after', 'append', 'appendTo', 'append_to', 'attr', 'base_url', 'before', 'children', 'clear', 'clone', 'closest', 'contents', 'copy', 'count', 'css', 'each', 'empty', 'encoding', 'end', 'eq', 'extend', 'filter', 'find', 'fn', 'hasClass', 'has_class', 'height', 'hide', 'html', 'index', 'insert', 'insertAfter', 'insertBefore', 'insert_after', 'insert_before', 'is_', 'items', 'length', 'make_links_absolute', 'map', 'next', 'nextAll', 'next_all', 'not_', 'outerHtml', 'outer_html', 'parent', 'parents', 'pop', 'prepend', 'prependTo', 'prepend_to', 'prev', 'prevAll', 'prev_all', 'remove', 'removeAttr', 'removeClass', 'remove_attr', 'remove_class', 'remove_namespaces', 'replaceAll', 'replaceWith', 'replace_all', 'replace_with', 'reverse', 'root', 'serialize', 'serializeArray', 'serializeDict', 'serializePairs', 'serialize_array', 'serialize_dict', 'serialize_pairs', 'show', 'siblings', 'size', 'sort', 'text', 'toggleClass', 'toggle_class', 'val', 'width', 'wrap', 'wrapAll', 'wrap_all', 'xhtml_to_html']\n"
     ]
    }
   ],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "print(dir(pq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html>\\n<html>\\n<head>\\n    <title>Example Domain</title>\\n\\n    <meta charset=\"utf-8\" />\\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\\n    <style type=\"text/css\">\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    </style>    \\n</head>\\n\\n<body>\\n<div>\\n    <h1>Example Domain</h1>\\n    <p>This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.</p>\\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get('http://www.example.com').text # conteúdo\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!doctype html>\\n<html>\\n<head>\\n    <title>Example Domain</title>\\n\\n    <meta charset=\"utf-8\" />\\n    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\\n    <style type=\"text/css\">\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    </style>    \\n</head>\\n\\n<body>\\n<div>\\n    <h1>Example Domain</h1>\\n    <p>This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.</p>\\n    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Podemos fazer com a urllib\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "response = urlopen('http://www.example.com').read()\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<html>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docTree = pq(response)\n",
    "docTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<html.no-js>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos olhar o site da Python\n",
    "pq('https://www.python.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyquery.pyquery.PyQuery"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Olhando o tipo\n",
    "site =pq('https://www.python.org')\n",
    "type(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<html>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq('https://www.samsclub.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos determinar qual será o parser: XML ou HTML\n",
    "doc = pq('http://www.example.com', parser='html') #using parser html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyquery.pyquery.PyQuery'>\n"
     ]
    }
   ],
   "source": [
    "doc = pq('<div><p>Testing block</p><p>Second block</p></div>')\n",
    "print(type(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "pagesource = open('pythonpage.html','r').read() # lendo localmente em Html\n",
    "print(type(pagesource))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyquery.pyquery.PyQuery"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = pq(pagesource)\n",
    "type(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos transversos, atributos, e pseud-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrando o elemento\n",
    "page('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Welcome to Python.org'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find('title').text() # Encontrando título e retornando o texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The official home of the Python Programming Language'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find('meta[name=\"description\"]').attr('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python programming language object oriented web free open source software license documentation download community'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find('meta[name=\"keywords\"]').attr('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Python 3.10.7'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buttons = page('a.button').html() # Retorna o Html para o elemento <a> com a classe 'button'\n",
    "buttons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `find()`: procura a expressão usando um seletor CSS.\n",
    "* `text()`: retorna o conteúdo do elemento em forma de string.\n",
    "* `attr()`: identifica o atributo e retorna o conteúdo.\n",
    "* `html()`: retorna o HTML da expressão avaliada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul.menu>, <ul.navigation.menu>, <ul.subnav.menu>, <ul.navigation.menu>, <ul.subnav.menu>, <ul.navigation.menu>, <ul.subnav.menu>, <ul.navigation.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul#launch-shell.launch-shell.menu>, <ul.slides.menu>, <ul.menu>, <ul.menu>, <ul.menu>, <ul#container.sitemap.navigation.menu.do-not-print>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.subnav.menu>, <ul.footer-links.navigation.menu.do-not-print>]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page('ul.menu') # Elemento com atributo \"menu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul.menu>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page('ul:first') # O primeiro elemento <ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page('a:first') # O primeiro elemento <a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul.footer-links.navigation.menu.do-not-print>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page('ul:last') # O último elemento <ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `:first:` Retorna a primeira ocorrência do elemento \n",
    "* `:last:` retorna a última ocorrência do elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1.site-headline>, <h4>, <h4>, <h3>, <h4>, <h4>, <h4>, <h4>, <h1>, <h1>, <h1>, <h1>, <h1>, <h2.widget-title>, <h2.widget-title>, <h2.widget-title>, <h2.widget-title>, <h2.widget-title>, <h2.widget-title>, <h2.widget-title>, <h2.widget-title>, <h2.widget-title>, <h2.widget-title>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vamos observar a implementação de um :pseudo-elemento.\n",
    "\n",
    "page(':header') # encontrados todos os Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<input#id-search-field.search-field>, <button#submit.search-button>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page(':input') # Encontrando todos os inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta>, <meta>, <link>, <link>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <script>, <script>, <script>, <link>, <link>, <link>, <link>, <link>, <link>, <link>, <link>, <link>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <link>, <link>, <link>, <link>, <link>, <img.python-logo>, <span.icon-search>, <input#id-search-field.search-field>, <span.breaker>, <span.icon-facebook>, <span.icon-twitter>, <span.icon-freenode>, <br>, <span.breaker>, <span.icon-get-started>, <span.icon-download>, <span.icon-documentation>, <span.icon-jobs>, <span.icon-news>, <span.icon-calendar>, <span.icon-success-stories>, <span.icon-python>, <span.icon-feed>, <div.python-logo>, <span#python-status-indicator.python-status-indicator-maintenance>, <script>, <script>, <script>, <script>, <script>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page(':empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta>, <link>, <meta>, <meta>, <meta>, <meta>, <meta>, <script>, <link>, <link>, <link>, <link>, <link>, <meta>, <meta>, <meta>, <meta>, <meta>, <meta>, <link>, <link>, <img.python-logo>, <input#id-search-field.search-field>, <span.icon-facebook>, <span.icon-freenode>, <span.breaker>, <span.icon-download>, <span.icon-jobs>, <span.icon-calendar>, <span.icon-python>, <div.python-logo>, <script>, <script>, <script>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page(':empty:odd') # elementos vazios, porém só os impares são listados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `:header`: retorna os elementos headers (h1,h2,h3...) encontrados na página.\n",
    "* `:input`: retorna todos os elementos inputs.\n",
    "* `:empty`: retorna todos os elementos que não tem elementos-filhos.\n",
    "* `:odd`: retorna os elementos indexados como números ímpares.\n",
    "* `:even`: parecido com o `:odd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'menu'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find('ul:first').attr('class') # Class no primeiro elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.python.org/#content'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find('a:first').attr('href') # href do primeiro elemento <a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.python.org/#content'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find('a:eq(0)').attr('href') # href do primeiro elemento <a> no index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skip to content'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find('a:eq(0)').text() # texto do primeiro elemento <a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `:eq`: seleciona um índice particular avaliado como igual a.\n",
    "* `:lt`: avalia para `less than` do valor informado.\n",
    "* `:gt`: avalia como `greater than` do valor informado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p.download-buttons>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>, <p>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page('p:contains(\"Python\")') # Retorna os elementos <p> com o texto \"Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>, <p>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page('p:contains(\"python.org\")') # retorna os elementos <p> com o texto \"python.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jobs.python.org'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page('p:contains(\"python.org\")').eq(1).text() # Retorna o texto do 2º <p> elemento que contenha o texto \"python.org\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `:contains`: combina todos os elementos com o texto informado.\n",
    "* `:eq()`: retorna o elemento que foi encontrando no index informado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checando se tem uma classe: python-logo\n",
    "page('h1.site-headline:first a img').is_('.python-logo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checando se <img> tem uma class='python-logo'\n",
    "page('h1.site-headline:first a img').has_class('python-logo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `is_()`: aceita um seletor como argumento e retorna `true` caso encontre a combinação ou `false` se for o contrário.\n",
    "* `has_class()`: retorna `true` se o seletor combina com a classe provida. Só é utilizada para *class*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interando "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['application-name', 'apple-mobile-web-app-title']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos procurar os atributos <name> e <property> que estão na tag <meta>\n",
    "# Vamos usar uma *List Comprehesion*\n",
    "\n",
    "meta = page.find('meta[content*=\"Python.org\"]')\n",
    "[item.attr('name') for item in meta.items() if item.attr('name') is not None] # Listando os atributos <name>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['og:site_name', 'og:title']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listando os atributos\n",
    "[item.attr('property') for item in meta.items() if item.attr('property') is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Facebook', 'Twitter', 'Chat on IRC']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social = page.find('a:contains(\"Socialize\") + ul.subnav li a')\n",
    "[item.text() for item in social.items() if item.text() is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.facebook.com/pythonlang?fref=ts',\n",
       " 'https://twitter.com/ThePSF',\n",
       " 'https://www.python.org/community/irc/']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item.attr('href') for item in social.items() if item.attr('href') is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Django', 'Pyramid', 'Bottle', 'Tornado', 'Flask', 'web2py']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webdevs = page.find('div.applications-widget:first ul.menu li:contains(\"Web Development\") a')\n",
    "[item.text() for item in webdevs.items() if item.text() is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturar uma lista de eventos.\n",
    "eventsList = []\n",
    "upcomingevents = page.find('div.event-widget ul.menu li')\n",
    "for event in upcomingevents.items():\n",
    "    time = event.find('time').text()\n",
    "    url  = event.find('a[href*=\"events/python\"]').attr('href')\n",
    "    title = event.find('a[href*=\"events/python\"]').text()\n",
    "    eventsList.append([time,title,url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-09-30', 'PyConEs - Granada', 'https://www.python.org/events/python-events/1274/']\n",
      "['2022-10-05', 'Crafting Software Series', 'https://www.python.org/events/python-user-group/1336/']\n",
      "['2022-10-10', 'PyCon MEA @ Global DevSlam 2022', 'https://www.python.org/events/python-events/1270/']\n",
      "['2022-10-13', 'PyCon ZA 2022', 'https://www.python.org/events/python-events/1275/']\n",
      "['2022-10-13', 'PyCon Ghana 2022', 'https://www.python.org/events/python-events/1330/']\n"
     ]
    }
   ],
   "source": [
    "for event in eventsList:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.7  ::  https://www.python.org/ftp/python/3.10.7/python-3.10.7-macos11.pkg\n",
      "Python 3.10.7  ::  https://www.python.org/ftp/python/3.10.7/Python-3.10.7.tar.xz\n",
      "Python 3.10.7  ::  https://www.python.org/ftp/python/3.10.7/python-3.10.7-amd64.exe\n",
      "View the full list of downloads  ::  https://www.python.org/downloads/operating-systems/\n",
      "Python Docs  ::  http://docs.python.org/3/\n",
      ">_ Launch Interactive Shell  ::  https://www.python.org/shell/\n",
      "Become a Member  ::  https://www.python.org/users/membership/\n",
      "Donate to the PSF  ::  https://www.python.org/psf/donations/\n"
     ]
    }
   ],
   "source": [
    "# iterando com o find() e o items()\n",
    "\n",
    "buttons = page.find('a.button')\n",
    "for item in buttons.items():\n",
    "    print(item.text(), ' :: ',item.attr('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.7  ::  https://www.python.org/ftp/python/3.10.7/Python-3.10.7.tar.xz\n",
      "View the full list of downloads  ::  https://www.python.org/downloads/operating-systems/\n",
      ">_ Launch Interactive Shell  ::  https://www.python.org/shell/\n",
      "Donate to the PSF  ::  https://www.python.org/psf/donations/\n"
     ]
    }
   ],
   "source": [
    "buttons = page.find('a.button:odd') # Indexando os índices impares\n",
    "for item in buttons.items():\n",
    "    print(item.text(), ' :: ',item.attr('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.7  ::  https://www.python.org/ftp/python/3.10.7/python-3.10.7-macos11.pkg\n",
      "Python 3.10.7  ::  https://www.python.org/ftp/python/3.10.7/python-3.10.7-amd64.exe\n",
      "Python Docs  ::  http://docs.python.org/3/\n",
      "Become a Member  ::  https://www.python.org/users/membership/\n"
     ]
    }
   ],
   "source": [
    "buttons = page.find('a.button:even') # Indexando os índices pares\n",
    "for item in buttons.items():\n",
    "    print(item.text(), ' :: ',item.attr('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping usando *pyquery*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagina de Notícias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "import requests\n",
    "dataSet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de Artigos encontrados: 15\n",
      "Hora: 11:40\n",
      "Título: DECISÃO: Filha de imigrante haitiano pode ingressar no Brasil sem visto\n",
      "Hora: 09:15\n",
      "Título: INSTITUCIONAL: TRF1 dispõe sobre a especialização de varas federais para processar e julgar crimes por atos de violência político\n",
      "Hora: 09:00\n",
      "Título: INSTITUCIONAL: Cibersegurança e ataques cibernéticos são assuntos da última palestra da Semana da Segurança da Informação\n",
      "Hora: 08:00\n",
      "Título: INSTITUCIONAL: Vem aí a campanha do Inventário Anual 2022 do TRF1\n",
      "Hora: 07:40\n",
      "Título: INSTITUCIONAL: Acompanhe o programa Inteiro Teor sobre acessibilidade dos serviços públicos\n",
      "Hora: 17:17\n",
      "Título: DECISÃO: RPVs e precatórios não retirados há mais de dois anos pelo credor podem ser requisitados de novo e não se sujeitam a prazo prescricional\n",
      "Hora: 15:50\n",
      "Título: DECISÃO: Dono de veículo usado por terceiro em delito deve pedir a restituição do bem onde iniciou o processo já que não houve decretação de sua perda\n",
      "Hora: 15:19\n",
      "Título: DECISÃO: Depósito em processo judicial para garantir execução da causa pode ser parcial desde que seja comprovada insuficiência econômica\n",
      "Hora: 14:32\n",
      "Título: DECISÃO: Servidor que responde a processo administrativo disciplinar não pode ter salário suspenso\n",
      "Hora: 13:34\n",
      "Título: DECISÃO:  É de cinco anos o prazo para reaver bem apreendido por infração ambiental\n",
      "Hora: 09:00\n",
      "Título: INSTITUCIONAL: Relatório apresenta perspectiva positiva para o exercício financeiro da Justiça Federal em 2023\n",
      "Hora: 08:55\n",
      "Título: INSTITUCIONAL: Analista de segurança aponta riscos da internet e dá dicas de como se proteger de golpes e ataques\n",
      "Hora: 08:50\n",
      "Título: DECISÃO: Distribuidoras de energia não podem ser responsabilizadas pela interrupção na prestação dos serviços quando o problema for do transmissor\n",
      "Hora: 08:35\n",
      "Título: INSTITUCIONAL: Semana do Coração: Cardiologista alerta para o cuidado precoce como prevenção de doenças cardiovasculares\n",
      "Hora: 07:40\n",
      "Título: INSTITUCIONAL:  Tribunais estão engajados na Semana da Conciliação que ocorrerá entre 7 e 11 de novembro\n",
      "\n",
      "Total de Artigos encontrados: 15\n",
      "Hora: 15:12\n",
      "Título: DECISÃO: Empresa de seguros privados é obrigada a contribuir para Pis/Cofins sobre todas as receitas independentemente da denominação\n",
      "Hora: 13:48\n",
      "Título: INSTITUCIONAL: Reint1 debate as demandas predatórias de vícios construtivos e anuncia a criação do Fórum da Moradia Digna da 1ª Região\n",
      "Hora: 13:42\n",
      "Título: INSTITUCIONAL: Acompanhe a palestra on\n",
      "Hora: 12:30\n",
      "Título: DECISÃO: É indevida a cobrança de IPTU contra a Caixa de imóvel transferido a particular por meio de programa de arrendamento residencial\n",
      "Hora: 09:15\n",
      "Título: INSTITUCIONAL: Mutirão agiliza o andamento de mais de 7,9 mil processos na Justiça Federal em Santarém/PA\n",
      "Hora: 08:55\n",
      "Título: INSTITUCIONAL: TRF1 promove encontros para demonstração do Sistema de Análise Legal Inteligente – ALEI\n",
      "Hora: 08:35\n",
      "Título: INSTITUCIONAL: Questionário CNJ que vai medir o nível de maturidade dos órgãos do Judiciário na área de TI pode ser respondido até sexta\n",
      "Hora: 08:27\n",
      "Título: DECISÃO: Conselho de Arquitetura e Urbanismo não pode negar registro profissional a aluna graduada por meio de ensino a distância\n",
      "Hora: 07:40\n",
      "Título: INSTITUCIONAL: Internet segura para todas as idades é o tema da 2ª palestra da Semana da Segurança da Informação da Justiça Federal da 1ª Região. Participe!\n",
      "Hora: 17:25\n",
      "Título: DECISÃO: Desistência de ação previdenciária não está condicionada à renúncia ao benefício\n",
      "Hora: 16:54\n",
      "Título: DECISÃO: Simples referência a nome de prefeito em inquérito não indica que o delito tenha sido praticado pela autoridade com foro especial\n",
      "Hora: 14:52\n",
      "Título: DECISÃO:  Resgate não solicitado de aplicação financeira gera o dever de banco indenizar correntista\n",
      "Hora: 13:48\n",
      "Título: DECISÃO: TRF1 julga que servidores inativos não podem receber gratificação e vantagem ao mesmo tempo\n",
      "Hora: 10:50\n",
      "Título: DECISÃO: Regras do CPC que preveem suspensão dos prazos referentes ao recesso forense não se aplicam ao processo penal\n",
      "Hora: 09:59\n",
      "Título: DECISÃO: É crime brasileiro apresentar documento falso do exterior\n",
      "\n",
      "Total de artigos: 30\n"
     ]
    }
   ],
   "source": [
    "sourceUrl = 'https://portal.trf1.jus.br/portaltrf1/comunicacao-social/imprensa/noticias/noticias/'\n",
    "\n",
    "def read_url(url):\n",
    "    \"\"\"\n",
    "    Lê uma dada url e retorna um objeto pyquery para uma pagina de conteúdo\n",
    "    \"\"\"\n",
    "    pageSource = requests.get(url).content\n",
    "    return pq(pageSource)\n",
    "\n",
    "# Vamos iterar sobre todas as páginas\n",
    "def get_details(page):\n",
    "    response = read_url(page)\n",
    "    articles = response.find('form[method=\"post\"] div ul')\n",
    "    print(f'\\nTotal de Artigos encontrados: {len(articles)}')\n",
    "    for page in articles.find('li a'):\n",
    "        hora = page.text.split(\"-\")[0].strip()\n",
    "        titulo = page.text.split('-')[1].strip()\n",
    "        print(f\"Hora: {hora}\\nTítulo: {titulo}\")\n",
    "        dataSet.append([hora, titulo])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pagesUrls = [sourceUrl + f'list-pagination/%s.htm' % i for i in range(1,3)]\n",
    "    for page in pagesUrls:\n",
    "        get_details(page)\n",
    "    print(f\"\\nTotal de artigos: {len(dataSet)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 2 - Raspando informações de links aninhados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Quotes found : 10  in Page:  1\n",
      "\n",
      "Total Quotes found : 1  in Page:  2\n",
      "\n",
      "Total de citações coletadas: 11.\n",
      "[{'quotes_tags': 'aliteracy,books,classic,humor', 'author_url': 'http://quotes.toscrape.com/author/Jane-Austen', 'author_name': 'Jane Austen', 'born_date': 'December 16, 1775', 'born_location': 'Steventon Rectory, Hampshire, The United Kingdom', 'quote_title': '“The person, be it gentleman or lady, who has not '}, {'quotes_tags': 'books,contentment,friends,friendship,life', 'author_url': 'http://quotes.toscrape.com/author/Mark-Twain', 'author_name': 'Mark Twain', 'born_date': 'November 30, 1835', 'born_location': 'Florida, Missouri, The United States', 'quote_title': '“Good friends, good books, and a sleepy conscience'}, {'quotes_tags': 'books,library', 'author_url': 'http://quotes.toscrape.com/author/Jorge-Luis-Borges', 'author_name': 'Jorge Luis Borges', 'born_date': 'August 24, 1899', 'born_location': 'Buenos Aires, Argentina', 'quote_title': '“I have always imagined that Paradise will be a ki'}, {'quotes_tags': 'books,inspirational,reading,tea', 'author_url': 'http://quotes.toscrape.com/author/C-S-Lewis', 'author_name': 'C.S. Lewis', 'born_date': 'November 29, 1898', 'born_location': 'Belfast, Ireland', 'quote_title': '“You can never get a cup of tea large enough or a '}, {'quotes_tags': 'books,thought', 'author_url': 'http://quotes.toscrape.com/author/Haruki-Murakami', 'author_name': 'Haruki Murakami', 'born_date': 'January 12, 1949', 'born_location': 'Kyoto, Japan', 'quote_title': '“If you only read the books that everyone else is '}, {'quotes_tags': 'books,friends,novelist-quotes', 'author_url': 'http://quotes.toscrape.com/author/Ernest-Hemingway', 'author_name': 'Ernest Hemingway', 'born_date': 'July 21, 1899', 'born_location': 'Oak Park, Illinois, The United States', 'quote_title': '“There is no friend as loyal as a book.”'}, {'quotes_tags': 'authors,books,literature,reading,writing', 'author_url': 'http://quotes.toscrape.com/author/J-D-Salinger', 'author_name': 'J.D. Salinger', 'born_date': 'January 01, 1919', 'born_location': 'Manhattan, New York, The United States', 'quote_title': '“What really knocks me out is a book that, when yo'}, {'quotes_tags': 'books,classic,reading', 'author_url': 'http://quotes.toscrape.com/author/Mark-Twain', 'author_name': 'Mark Twain', 'born_date': 'November 30, 1835', 'born_location': 'Florida, Missouri, The United States', 'quote_title': \"“′Classic′ - a book which people praise and don't \"}, {'quotes_tags': 'books,library,reading', 'author_url': 'http://quotes.toscrape.com/author/Jane-Austen', 'author_name': 'Jane Austen', 'born_date': 'December 16, 1775', 'born_location': 'Steventon Rectory, Hampshire, The United Kingdom', 'quote_title': '“I declare after all there is no enjoyment like re'}, {'quotes_tags': 'books,children,difficult,grown-ups,write,writers,writing', 'author_url': 'http://quotes.toscrape.com/author/Madeleine-LEngle', 'author_name': \"Madeleine L'Engle\", 'born_date': 'November 29, 1918', 'born_location': 'New York City, New York, The United States', 'quote_title': '“You have to write the book that wants to be writt'}, {'quotes_tags': 'books,mind', 'author_url': 'http://quotes.toscrape.com/author/George-R-R-Martin', 'author_name': 'George R.R. Martin', 'born_date': 'September 20, 1948', 'born_location': 'Bayonne, New Jersey, The United States', 'quote_title': '“... a mind needs books as a sword needs a whetsto'}]\n",
      "Jane Austen nasceu em December 16, 1775, em Steventon Rectory, Hampshire, The United Kingdom\n",
      "Mark Twain nasceu em November 30, 1835, em Florida, Missouri, The United States\n",
      "Jorge Luis Borges nasceu em August 24, 1899, em Buenos Aires, Argentina\n",
      "C.S. Lewis nasceu em November 29, 1898, em Belfast, Ireland\n",
      "Haruki Murakami nasceu em January 12, 1949, em Kyoto, Japan\n",
      "Ernest Hemingway nasceu em July 21, 1899, em Oak Park, Illinois, The United States\n",
      "J.D. Salinger nasceu em January 01, 1919, em Manhattan, New York, The United States\n",
      "Mark Twain nasceu em November 30, 1835, em Florida, Missouri, The United States\n",
      "Jane Austen nasceu em December 16, 1775, em Steventon Rectory, Hampshire, The United Kingdom\n",
      "Madeleine L'Engle nasceu em November 29, 1918, em New York City, New York, The United States\n",
      "George R.R. Martin nasceu em September 20, 1948, em Bayonne, New Jersey, The United States\n"
     ]
    }
   ],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "\n",
    "sourceUrl = 'http://quotes.toscrape.com/tag/books/'\n",
    "dataSet = []\n",
    "keys = ['quotes_tags', 'author_url', 'author_name','born_date', 'born_location', 'quote_title']\n",
    "\n",
    "def read_url(url):\n",
    "    # Retorna um objeto pyquery a partir de um url\n",
    "    pageSource = pq(url)\n",
    "    return pq(pageSource)\n",
    "\n",
    "def get_details(page):\n",
    "    \"\"\"read 'page' url and append list of queried items to dataSet\"\"\"\n",
    "    nextPage = True\n",
    "    pageNo = 1\n",
    "    while (nextPage):\n",
    "        response = read_url(page + 'page/' + str(pageNo))\n",
    "        if response.find(\"ul.pager:has('li.next')\"):\n",
    "            nextPage = True\n",
    "        else:\n",
    "            nextPage = False\n",
    "\n",
    "        quotes = response.find('.quote')\n",
    "        print(\"\\nTotal Quotes found :\", quotes.__len__(), ' in Page: ', pageNo)\n",
    "        for quote in quotes.items():\n",
    "            title = quote.find('[itemprop=\"text\"]:first').text()\n",
    "            author = quote.find('[itemprop=\"author\"]:first').text()\n",
    "            authorLink = quote.find('a[href*=\"/author/\"]:first').attr('href')\n",
    "            tags = quote.find('.tags [itemprop=\"keywords\"]').attr('content')\n",
    "            \n",
    "            if authorLink:\n",
    "                authorLink = 'http://quotes.toscrape.com' + authorLink\n",
    "                linkDetail = read_url(authorLink)\n",
    "                born_date = linkDetail.find('.author-born-date').text()\n",
    "                born_location = linkDetail.find('.author-born-location').text()\n",
    "                if born_location.startswith('in'):\n",
    "                    born_location = born_location.replace('in ','')\n",
    "            dataSet.append(dict(zip(keys,[tags,authorLink,author,born_date,born_location,title[0:50]])))\n",
    "        pageNo += 1\n",
    "\n",
    "if  __name__ == '__main__':\n",
    "    get_details(sourceUrl)\n",
    "    print(f'\\nTotal de citações coletadas: {len(dataSet)}.')\n",
    "    print(dataSet)\n",
    "    \n",
    "    for info in dataSet:\n",
    "        print(f\"{info['author_name']} nasceu em {info['born_date']}, em {info['born_location']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 3 - Extraindo o resultado dos playoffs da AHL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É uma grande tabela com vários campos.\n",
    "\n",
    "Nosso interesse é capturar os seguintes campos:\n",
    "\n",
    "`keys = ['year','month','day', 'game_date', 'team1', 'team1_score', 'team2','team2_score', 'game_status']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows found : 501\n",
      "\n",
      "Total Game Status, found : 359\n",
      "['1968', 'Apr', '3', '3-Apr-68', 'Buff', '2', 'Que', '4', 'W']\n",
      "['1968', 'Apr', '5', '5-Apr-68', 'Buff', '1', 'Que', '3', 'W']\n",
      "['1968', 'Apr', '9', '9-Apr-68', 'Que', '7', 'Buff', '10', 'L']\n",
      "['1968', 'Apr', '10', '10-Apr-68', 'Que', '4', 'Buff', '7', 'L']\n",
      "['1968', 'Apr', '12', '12-Apr-68', 'Buff', '1', 'Que', '3', 'W']\n",
      "['1968', 'Apr', '14', '14-Apr-68', 'Prv', '1', 'Que', '2', 'W']\n",
      "['1968', 'Apr', '17', '17-Apr-68', 'Prv', '2', 'Que', '6', 'W']\n",
      "['1968', 'Apr', '18', '18-Apr-68', 'Que', '1', 'Prv', '4', 'L']\n",
      "['1968', 'Apr', '29', '29-Apr-68', 'Que', '4', 'Prv', '3', 'W']\n",
      "['1968', 'Apr', '24', '24-Apr-68', 'Que', '2', 'Rch', '4', 'L']\n",
      "['1968', 'Apr', '26', '26-Apr-68', 'Que', '2', 'Rch', '4', 'L']\n",
      "['1968', 'Apr', '28', '28-Apr-68', 'Rch', '3', 'Que', '6', 'W']\n",
      "['1968', 'Apr', '30', '30-Apr-68', 'Rch', '5', 'Que', '6', 'W']\n",
      "['1968', 'May', '3', '3-May-68', 'Que', '3', 'Rch', '4', 'L']\n",
      "['1968', 'May', '4', '4-May-68', 'Rch', '4', 'Que', '2', 'L']\n",
      "['1969', 'Apr', '9', '9-Apr-69', 'Que', '1', 'Clv', '2', 'L']\n",
      "['1969', 'Apr', '12', '12-Apr-69', 'Que', '1', 'Clv', '2', 'L']\n",
      "['1969', 'Apr', '13', '13-Apr-69', 'Clv', '2', 'Que', '9', 'W']\n",
      "['1969', 'Apr', '15', '15-Apr-69', 'Clv', '1', 'Que', '5', 'W']\n",
      "['1969', 'Apr', '16', '16-Apr-69', 'Que', '3', 'Clv', '2', 'W']\n",
      "['1969', 'Apr', '18', '18-Apr-69', 'Que', '4', 'Prv', '5', 'L']\n",
      "['1969', 'Apr', '20', '20-Apr-69', 'Que', '4', 'Prv', '3', 'W']\n",
      "['1969', 'Apr', '21', '21-Apr-69', 'Prv', '3', 'Que', '4', 'W']\n",
      "['1969', 'Apr', '23', '23-Apr-69', 'Prv', '2', 'Que', '1', 'L']\n",
      "['1969', 'Apr', '25', '25-Apr-69', 'Que', '8', 'Prv', '1', 'W']\n",
      "['1969', 'Apr', '26', '26-Apr-69', 'Que', '0', 'Her', '4', 'L']\n",
      "['1969', 'Apr', '30', '30-Apr-69', 'Que', '3', 'Her', '4', 'L']\n",
      "['1969', 'May', '2', '2-May-69', 'Her', '4', 'Que', '3', 'L']\n",
      "['1969', 'May', '4', '4-May-69', 'Her', '3', 'Que', '6', 'W']\n",
      "['1969', 'May', '7', '7-May-69', 'Que', '0', 'Her', '3', 'L']\n",
      "['1970', 'Apr', '8', '8-Apr-70', 'Que', '2', 'Buff', '1', 'W']\n",
      "['1970', 'Apr', '10', '10-Apr-70', 'Que', '3', 'Buff', '4', 'L']\n",
      "['1970', 'Apr', '12', '12-Apr-70', 'Buff', '2', 'Que', '3', 'W']\n",
      "['1970', 'Apr', '15', '15-Apr-70', 'Buff', '3', 'Que', '2', 'L']\n",
      "['1970', 'Apr', '16', '16-Apr-70', 'Que', '0', 'Buff', '2', 'L']\n",
      "['1970', 'Apr', '18', '18-Apr-70', 'Buff', '3', 'Que', '1', 'L']\n",
      "['1971', 'Apr', '6', '6-Apr-71', 'Que', '3', 'Spf', '4', 'L']\n",
      "['1973', 'Apr', '4', '4-Apr-73', 'Rich', '1', 'Cin', '5', 'L']\n",
      "['1973', 'Apr', '6', '6-Apr-73', 'Rich', '3', 'Cin', '8', 'L']\n",
      "['1973', 'Apr', '8', '8-Apr-73', 'Cin', '7', 'Rich', '4', 'L']\n",
      "['1973', 'Apr', '10', '10-Apr-73', 'Cin', '5', 'Rich', '4', 'L']\n",
      "['1974', 'Apr', '5', '5-Apr-74', 'Rich', '2', 'Blt', '5', 'L']\n",
      "['1974', 'Apr', '6', '6-Apr-74', 'Blt', '0', 'Rich', '10', 'W']\n",
      "['1974', 'Apr', '9', '9-Apr-74', 'Rich', '2', 'Blt', '4', 'L']\n",
      "['1974', 'Apr', '10', '10-Apr-74', 'Blt', '6', 'Rich', '5', 'L']\n",
      "['1974', 'Apr', '9', '9-Apr-74', 'Rich', '6', 'Blt', '7', 'L']\n",
      "['1975', 'Apr', '3', '3-Apr-75', 'Her', '3', 'Ric', '5', 'W']\n",
      "['1975', 'Apr', '5', '5-Apr-75', 'Ric', '3', 'Her', '6', 'L']\n",
      "['1975', 'Apr', '6', '6-Apr-75', 'Her', '5', 'Ric', '2', 'L']\n",
      "['1975', 'Apr', '9', '9-Apr-75', 'Ric', '3', 'Her', '6', 'L']\n",
      "['1975', 'Apr', '11', '11-Apr-75', 'Her', '4', 'Ric', '5', 'W']\n",
      "['1975', 'Apr', '12', '12-Apr-75', 'Ric', '2', 'Her', '1', 'W']\n",
      "['1975', 'Apr', '16', '16-Apr-75', 'Her', '4', 'Ric', '3', 'L']\n",
      "['1976', 'Apr', '3', '3-Apr-76', 'NH', '1', 'Ric', '4', 'W']\n",
      "['1976', 'Apr', '4', '4-Apr-76', 'Ric', '2', 'NH', '1', 'W']\n",
      "['1976', 'Apr', '7', '7-Apr-76', 'NH', '1', 'Ric', '4', 'W']\n",
      "['1976', 'Apr', '10', '10-Apr-76', 'Ric', '4', 'Her', '2', 'W']\n",
      "['1976', 'Apr', '11', '11-Apr-76', 'Her', '4', 'Ric', '3', 'L']\n",
      "['1976', 'Apr', '13', '13-Apr-76', 'Ric', '1', 'Her', '3', 'L']\n",
      "['1976', 'Apr', '16', '16-Apr-76', 'Her', '7', 'Ric', '5', 'L']\n",
      "['1976', 'Apr', '17', '17-Apr-76', 'Ric', '1', 'Her', '2', 'L']\n",
      "['1978', 'Apr', '20', '20-Apr-78', 'NS', '5', 'Mne', '7', 'W']\n",
      "['1978', 'Apr', '23', '23-Apr-78', 'NS', '7', 'Mne', '5', 'L']\n",
      "['1978', 'Apr', '26', '26-Apr-78', 'Mne', '4', 'NS', '6', 'L']\n",
      "['1978', 'Apr', '28', '28-Apr-78', 'Mne', '3', 'NS', '0', 'W']\n",
      "['1978', 'Apr', '30', '30-Apr-78', 'NS', '2', 'Mne', '4', 'W']\n",
      "['1978', 'May', '1', '1-May-78', 'Mne', '1', 'NS', '7', 'L']\n",
      "['1978', 'May', '3', '3-May-78', 'NS', '1', 'Mne', '2', 'W']\n",
      "['1978', 'May', '5', '5-May-78', 'NH', '5', 'Mne', '2', 'L']\n",
      "['1978', 'May', '7', '7-May-78', 'NH', '1', 'Mne', '4', 'W']\n",
      "['1978', 'May', '12', '12-May-78', 'Mne', '8', 'NH', '4', 'W']\n",
      "['1978', 'May', '14', '14-May-78', 'Mne', '3', 'NH', '2', 'W']\n",
      "['1978', 'May', '15', '15-May-78', 'NH', '4', 'Mne', '5', 'W']\n",
      "['1979', 'Apr', '22', '22-Apr-79', 'NS', '2', 'Mne', '7', 'W']\n",
      "['1979', 'Apr', '24', '24-Apr-79', 'NS', '1', 'Mne', '4', 'W']\n",
      "['1979', 'Apr', '27', '27-Apr-79', 'Mne', '5', 'NS', '7', 'L']\n",
      "['1979', 'Apr', '28', '28-Apr-79', 'Mne', '5', 'NS', '2', 'W']\n",
      "['1979', 'Apr', '30', '30-Apr-79', 'NS', '3', 'Mne', '2', 'L']\n",
      "['1979', 'May', '2', '2-May-79', 'Mne', '5', 'NS', '3', 'W']\n",
      "['1979', 'May', '3', '3-May-79', 'NH', '3', 'Mne', '4', 'W']\n",
      "['1979', 'May', '6', '6-May-79', 'Mne', '5', 'NH', '3', 'W']\n",
      "['1979', 'May', '9', '9-May-79', 'NH', '2', 'Mne', '5', 'W']\n",
      "['1979', 'May', '11', '11-May-79', 'Mne', '8', 'NH', '4', 'W']\n",
      "['1980', 'Apr', '10', '10-Apr-80', 'Mne', '3', 'NS', '2', 'W']\n",
      "['1980', 'Apr', '12', '12-Apr-80', 'Mne', '2', 'NS', '3', 'L']\n",
      "['1980', 'Apr', '15', '15-Apr-80', 'NS', '0', 'Mne', '7', 'W']\n",
      "['1980', 'Apr', '17', '17-Apr-80', 'NS', '5', 'Mne', '6', 'W']\n",
      "['1980', 'Apr', '19', '19-Apr-80', 'Mne', '2', 'NS', '4', 'L']\n",
      "['1980', 'Apr', '21', '21-Apr-80', 'NS', '3', 'Mne', '7', 'W']\n",
      "['1980', 'Apr', '23', '23-Apr-80', 'Mne', '2', 'NB', '3', 'L']\n",
      "['1980', 'Apr', '28', '28-Apr-80', 'NB', '3', 'Mne', '2', 'L']\n",
      "['1980', 'Apr', '30', '30-Apr-80', 'Mne', '6', 'NB', '2', 'W']\n",
      "['1980', 'May', '3', '3-May-80', 'MB', '5', 'Mne', '7', 'W']\n",
      "['1980', 'May', '4', '4-May-80', 'Mne', '2', 'NB', '5', 'L']\n",
      "['1980', 'May', '6', '6-May-80', 'NB', '3', 'Mne', '2', 'L']\n",
      "['1981', 'Apr', '8', '8-Apr-81', 'Spr', '1', 'Mne', '3', 'W']\n",
      "['1981', 'Apr', '10', '10-Apr-81', 'Spr', '2', 'Mne', '7', 'W']\n",
      "['1981', 'Apr', '12', '12-Apr-81', 'Mne', '4', 'Spr', '7', 'L']\n",
      "['1981', 'Apr', '15', '15-Apr-81', 'Mne', '3', 'Spr', '10', 'L']\n",
      "['1981', 'Apr', '17', '17-Apr-81', 'Spr', '2', 'Mne', '6', 'W']\n",
      "['1981', 'Apr', '18', '18-Apr-81', 'Mne', '2', 'Spr', '9', 'L']\n",
      "['1981', 'Apr', '22', '22-Apr-81', 'Spr', '3', 'Mne', '7', 'W']\n",
      "['1981', 'Apr', '25', '25-Apr-81', 'NB', '1', 'Mne', '4', 'W']\n",
      "['1981', 'Apr', '27', '27-Apr-81', 'NB', '5', 'Mne', '4', 'L']\n",
      "['1981', 'Apr', '29', '29-Apr-81', 'Mne', '3', 'NB', '2', 'W']\n",
      "['1981', 'May', '1', '1-May-81', 'Mne', '2', 'NB', '6', 'L']\n",
      "['1981', 'May', '3', '3-May-81', 'NB', '4', 'Mne', '5', 'W']\n",
      "['1981', 'May', '6', '6-May-81', 'Mne', '1', 'NB', '7', 'L']\n",
      "['1981', 'May', '8', '8-May-81', 'NB', '3', 'Mne', '4', 'W']\n",
      "['1981', 'May', '11', '11-May-81', 'Adr', '2', 'Mne', '3', 'W']\n",
      "['1981', 'May', '13', '13-May-81', 'Adr', '4', 'Mne', '3', 'L']\n",
      "['1981', 'May', '14', '14-May-81', 'Mne', '0', 'Adr', '1', 'L']\n",
      "['1981', 'May', '16', '16-May-81', 'Mne', '10', 'Adr', '1', 'W']\n",
      "['1981', 'May', '18', '18-May-81', 'Adr', '6', 'Mne', '4', 'L']\n",
      "['1981', 'May', '20', '20-May-81', 'Mne', '2', 'Adr', '5', 'L']\n",
      "['1982', 'Apr', '8', '8-Apr-82', 'NS', '7', 'Mne', '6', 'L']\n",
      "['1982', 'Apr', '9', '9-Apr-82', 'NS', '3', 'Mne', '4', 'W']\n",
      "['1982', 'Apr', '12', '12-Apr-82', 'Mne', '1', 'NS', '3', 'L']\n",
      "['1982', 'Apr', '14', '14-Apr-82', 'Mne', '3', 'NS', '6', 'L']\n",
      "['1983', 'Apr', '8', '8-Apr-83', 'Mne', '3', 'NS', '5', 'L']\n",
      "['1983', 'Apr', '10', '10-Apr-83', 'Mne', '3', 'NS', '1', 'W']\n",
      "['1983', 'Apr', '14', '14-Apr-83', 'NS', '1', 'Mne', '3', 'W']\n",
      "['1983', 'Apr', '15', '15-Apr-83', 'NS', '3', 'Mne', '2', 'L']\n",
      "['1983', 'Apr', '17', '17-Apr-83', 'NS', '5', 'Mne', '2', 'L']\n",
      "['1983', 'Apr', '19', '19-Apr-83', 'Mne', '5', 'NS', '4', 'W']\n",
      "['1983', 'Apr', '21', '21-Apr-83', 'Mne', '4', 'NS', '1', 'W']\n",
      "['1983', 'Apr', '26', '26-Apr-83', 'Mne', '2', 'Fre', '1', 'W']\n",
      "['1983', 'Apr', '28', '28-Apr-83', 'Mne', '3', 'Fre', '6', 'L']\n",
      "['1983', 'May', '1', '1-May-83', 'Fre', '2', 'Mne', '3', 'W']\n",
      "['1983', 'May', '4', '4-May-83', 'Fre', '2', 'Mne', '3', 'W']\n",
      "['1983', 'May', '7', '7-May-83', 'Fre', '7', 'Mne', '4', 'L']\n",
      "['1983', 'May', '9', '9-May-83', 'Mne', '4', 'Fre', '3', 'W']\n",
      "['1983', 'May', '13', '13-May-83', 'Mne', '2', 'Rch', '3', 'L']\n",
      "['1983', 'May', '15', '15-May-83', 'Mne', '3', 'Rch', '4', 'L']\n",
      "['1983', 'May', '17', '17-May-83', 'Rch', '5', 'Mne', '1', 'L']\n",
      "['1983', 'May', '19', '19-May-83', 'Rch', '3', 'Mne', '1', 'L']\n",
      "['1984', 'Apr', '3', '3-Apr-84', 'Spr', '2', 'Balt', '10', 'L']\n",
      "['1984', 'Apr', '4', '4-Apr-84', 'Spr', '3', 'Balt', '5', 'L']\n",
      "['1984', 'Apr', '6', '6-Apr-84', 'Balt', '8', 'Spr', '7', 'L']\n",
      "['1984', 'Apr', '7', '7-Apr-84', 'Balt', '5', 'Spr', '3', 'L']\n",
      "['1986', 'Apr', '9', '9-Apr-86', 'NH', '4', 'Her', '3', 'L']\n",
      "['1986', 'Apr', '12', '12-Apr-86', 'NH', '0', 'Her', '6', 'W']\n",
      "['1986', 'Apr', '13', '13-Apr-86', 'Her', '4', 'NH', '3', 'W']\n",
      "['1986', 'Apr', '15', '15-Apr-86', 'Her', '5', 'NH', '4', 'W']\n",
      "['1986', 'Apr', '16', '16-Apr-86', 'NH', '4', 'Her', '5', 'W']\n",
      "['1986', 'Apr', '23', '23-Apr-86', 'StC', '6', 'Her', '4', 'L']\n",
      "['1986', 'Apr', '26', '26-Apr-86', 'StC', '2', 'Her', '4', 'W']\n",
      "['1986', 'Apr', '28', '28-Apr-86', 'Her', '3', 'StC', '1', 'W']\n",
      "['1986', 'Apr', '30', '30-Apr-86', 'Her', '4', 'StC', '2', 'W']\n",
      "['1986', 'May', '3', '3-May-86', 'StC', '2', 'Her', '1', 'L']\n",
      "['1986', 'May', '5', '5-May-86', 'Her', '2', 'StC', '7', 'L']\n",
      "['1986', 'May', '7', '7-May-86', 'StC', '2', 'Her', '4', 'W']\n",
      "['1986', 'May', '10', '10-May-86', 'Adr', '4', 'Her', '5', 'W']\n",
      "['1986', 'May', '13', '13-May-86', 'Adr', '5', 'Her', '4', 'L']\n",
      "['1986', 'May', '15', '15-May-86', 'Her', '2', 'Adr', '5', 'L']\n",
      "['1986', 'May', '17', '17-May-86', 'Her', '3', 'Adr', '4', 'L']\n",
      "['1986', 'May', '18', '18-May-86', 'Adr', '1', 'Her', '2', 'W']\n",
      "['1986', 'May', '21', '21-May-86', 'Her', '3', 'Adr', '7', 'L']\n",
      "['1987', 'Apr', '9', '9-Apr-87', 'Her', '2', 'Rch', '5', 'L']\n",
      "['1987', 'Apr', '11', '11-Apr-87', 'Her', '4', 'Rch', '5', 'L']\n",
      "['1987', 'Apr', '12', '12-Apr-87', 'Rch', '1', 'Her', '3', 'W']\n",
      "['1987', 'Apr', '15', '15-Apr-87', 'Rch', '6', 'Her', '5', 'L']\n",
      "['1987', 'Apr', '17', '17-Apr-87', 'Her', '3', 'Rch', '7', 'L']\n",
      "['1988', 'Apr', '6', '6-Apr-88', 'Bng', '2', 'Her', '3', 'W']\n",
      "['1988', 'Apr', '8', '8-Apr-88', 'Her', '4', 'Bng', '1', 'W']\n",
      "['1988', 'Apr', '9', '9-Apr-88', 'Bng', '3', 'Her', '4', 'W']\n",
      "['1988', 'Apr', '11', '11-Apr-88', 'Her', '5', 'Bng', '8', 'W']\n",
      "['1988', 'Apr', '23', '23-Apr-88', 'Adr', '1', 'Her', '5', 'W']\n",
      "['1988', 'Apr', '25', '25-Apr-88', 'Adr', '2', 'Her', '6', 'W']\n",
      "['1988', 'Apr', '27', '27-Apr-88', 'Her', '6', 'Adr', '4', 'W']\n",
      "['1988', 'Apr', '28', '28-Apr-88', 'Her', '6', 'Adr', '5', 'W']\n",
      "['1988', 'May', '8', '8-May-88', 'Fred', '0', 'Her', '4', 'W']\n",
      "['1988', 'May', '9', '9-May-88', 'Fred', '2', 'Her', '5', 'W']\n",
      "['1988', 'May', '11', '11-May-88', 'Her', '5', 'Fred', '1', 'W']\n",
      "['1988', 'May', '12', '12-May-88', 'Her', '4', 'Fred', '2', 'W']\n",
      "['1989', 'Apr', '6', '6-Apr-89', 'Uti', '1', 'Her', '6', 'W']\n",
      "['1989', 'Apr', '8', '8-Apr-89', 'Uti', '1', 'Her', '6', 'W']\n",
      "['1989', 'Apr', '11', '11-Apr-89', 'Her', '2', 'Uti', '3', 'L']\n",
      "['1989', 'Apr', '13', '13-Apr-89', 'Her', '3', 'Uti', '2', 'W']\n",
      "['1989', 'Apr', '15', '15-Apr-89', 'Uti', '1', 'Her', '7', 'W']\n",
      "['1989', 'Apr', '6', '6-Apr-89', 'Uti', '1', 'Her', '6', 'W']\n",
      "['1989', 'Apr', '8', '8-Apr-89', 'Uti', '1', 'Her', '6', 'W']\n",
      "['1989', 'Apr', '11', '11-Apr-89', 'Her', '2', 'Uti', '3', 'L']\n",
      "['1989', 'Apr', '13', '13-Apr-89', 'Her', '3', 'Uti', '2', 'W']\n",
      "['1989', 'Apr', '15', '15-Apr-89', 'Uti', '1', 'Her', '7', 'W']\n",
      "['1989', 'Apr', '19', '19-Apr-89', 'Her', '4', 'Adr', '3', 'W']\n",
      "['1989', 'Apr', '21', '21-Apr-89', 'Her', '5', 'Adr', '3', 'W']\n",
      "['1989', 'Apr', '23', '23-Apr-89', 'Adr', '0', 'Her', '1', 'W']\n",
      "['1989', 'Apr', '24', '24-Apr-89', 'Adr', '6', 'Her', '3', 'L']\n",
      "['1989', 'Apr', '26', '26-Apr-89', 'Her', '2', 'Adr', '3', 'L']\n",
      "['1989', 'Apr', '28', '28-Apr-89', 'Adr', '4', 'Her', '2', 'L']\n",
      "['1991', 'Apr', '2', '2-Apr-91', 'Adr', '3', 'Her', '2', 'L']\n",
      "['1991', 'Apr', '3', '3-Apr-91', 'Adr', '1', 'Her', '11', 'W']\n",
      "['1991', 'Apr', '3', '3-Apr-91', 'Adr', '0', 'Her', '1', 'W']\n",
      "['1991', 'Apr', '5', '5-Apr-91', 'Her', '4', 'Rch', '3', 'W']\n",
      "['1991', 'Apr', '7', '7-Apr-91', 'Her', '1', 'Rch', '4', 'L']\n",
      "['1991', 'Apr', '9', '9-Apr-91', 'Rch', '7', 'Her', '4', 'L']\n",
      "['1991', 'Apr', '11', '11-Apr-91', 'Rch', '5', 'Her', '2', 'L']\n",
      "['1991', 'Apr', '13', '13-Apr-91', 'Her', '1', 'Rch', '4', 'L']\n",
      "['1992', 'Apr', '8', '8-Apr-92', 'Her', '3', 'Rch', '4', 'L']\n",
      "['1992', 'Apr', '10', '10-Apr-92', 'Her', '3', 'Rch', '4', 'L']\n",
      "['1992', 'Apr', '12', '12-Apr-92', 'Rch', '5', 'Her', '3', 'L']\n",
      "['1992', 'Apr', '15', '15-Apr-92', 'Rch', '4', 'Her', '5', 'L']\n",
      "['1992', 'Apr', '17', '17-Apr-92', 'Her', '2', 'Rch', '1', 'W']\n",
      "['1992', 'Apr', '18', '18-Apr-92', 'Rch', '2', 'Her', '0', 'L']\n",
      "['1994', 'Apr', '13', '13-Apr-94', 'Rch', '3', 'Her', '5', 'W']\n",
      "['1994', 'Apr', '15', '15-Apr-94', 'Rch', '4', 'Her', '5', 'W']\n",
      "['1994', 'Apr', '17', '17-Apr-94', 'Her', '4', 'Rch', '1', 'W']\n",
      "['1994', 'Apr', '20', '20-Apr-94', 'Her', '4', 'Rch', '3', 'W']\n",
      "['1994', 'Apr', '27', '27-Apr-94', 'Cor', '2', 'Her', '4', 'W']\n",
      "['1994', 'Apr', '29', '29-Apr-94', 'Cor', '5', 'Her', '1', 'L']\n",
      "['1994', 'May', '2', '2-May-94', 'Her', '3', 'Cor', '1', 'W']\n",
      "['1994', 'May', '4', '4-May-94', 'Her', '3', 'Cor', '4', 'L']\n",
      "['1994', 'May', '6', '6-May-94', 'Her', '2', 'Cor', '3', 'L']\n",
      "['1994', 'May', '8', '8-May-94', 'Cor', '3', 'Her', '4', 'W']\n",
      "['1994', 'May', '11', '11-May-94', 'Cor', '4', 'Her', '3', 'L']\n",
      "['1995', 'Apr', '13', '13-Apr-95', 'Her', '3', 'Cor', '4', 'L']\n",
      "['1995', 'Apr', '15', '15-Apr-95', 'Her', '3', 'Cor', '4', 'L']\n",
      "['1995', 'Apr', '18', '18-Apr-95', 'Cor', '3', 'Her', '2', 'L']\n",
      "['1995', 'Apr', '20', '20-Apr-95', 'Cor', '2', 'Her', '4', 'W']\n",
      "['1995', 'Apr', '21', '21-Apr-95', 'Cor', '3', 'Her', '6', 'W']\n",
      "['1995', 'Apr', '23', '23-Apr-95', 'Her', '1', 'Cor', '2', 'L']\n",
      "['1996', 'Apr', '19', '19-Apr-96', 'Balt', '5', 'Her', '2', 'L']\n",
      "['1996', 'Apr', '20', '20-Apr-96', 'Her', '2', 'Balt', '3', 'L']\n",
      "['1996', 'Apr', '24', '24-Apr-96', 'Balt', '5', 'Her', '7', 'W']\n",
      "['1996', 'Apr', '26', '26-Apr-96', 'Her', '4', 'Balt', '2', 'W']\n",
      "['1996', 'Apr', '28', '28-Apr-96', 'Balt', '4', 'Her', '3', 'L']\n",
      "['1997', 'Apr', '19', '19-Apr-97', 'Blt', '2', 'Phil', '4', 'W']\n",
      "['1997', 'Apr', '20', '20-Apr-97', 'Blt', '1', 'Phil', '7', 'W']\n",
      "['1997', 'Apr', '23', '23-Apr-97', 'Phil', '4', 'Blt', '2', 'W']\n",
      "['1997', 'May', '1', '1-May-97', 'Her', '5', 'Phil', '3', 'L']\n",
      "['1997', 'May', '3', '3-May-97', 'Her', '4', 'Phil', '7', 'W']\n",
      "['1997', 'May', '4', '4-May-97', 'Phil', '2', 'Her', '4', 'L']\n",
      "['1997', 'May', '7', '7-May-97', 'Phil', '3', 'Her', '2', 'W']\n",
      "['1997', 'May', '10', '10-May-97', 'Her', '0', 'Phil', '3', 'W']\n",
      "['1997', 'May', '11', '11-May-97', 'Phil', '2', 'Her', '3', 'L']\n",
      "['1997', 'May', '14', '14-May-97', 'Her', '3', 'Phil', '2', 'L']\n",
      "['1998', 'Apr', '14', '14-Apr-98', 'Rch', '2', 'Phil', '5', 'W']\n",
      "['1998', 'Apr', '16', '16-Apr-98', 'Rch', '5', 'Phil', '2', 'L']\n",
      "['1998', 'Apr', '18', '18-Apr-98', 'Phil', '5', 'Rch', '2', 'W']\n",
      "['1998', 'Apr', '19', '19-Apr-98', 'Phil', '6', 'Rch', '3', 'W']\n",
      "['1998', 'Apr', '26', '26-Apr-98', 'Her', '2', 'Phil', '3', 'W']\n",
      "['1998', 'Apr', '29', '29-Apr-98', 'Her', '3', 'Phil', '5', 'W']\n",
      "['1998', 'May', '2', '2-May-98', 'Phil', '4', 'Her', '2', 'W']\n",
      "['1998', 'May', '3', '3-May-98', 'Phil', '3', 'Her', '0', 'W']\n",
      "['1998', 'May', '15', '15-May-98', 'Alb', '2', 'Phil', '4', 'W']\n",
      "['1998', 'May', '17', '17-May-98', 'Alb', '2', 'Phil', '1', 'L']\n",
      "['1998', 'May', '20', '20-May-98', 'Phil', '2', 'Alb', '0', 'W']\n",
      "['1998', 'May', '22', '22-May-98', 'Phil', '7', 'Alb', '3', 'W']\n",
      "['1998', 'May', '23', '23-May-98', 'Alb', '4', 'Phil', '3', 'L']\n",
      "['1998', 'May', '25', '25-May-98', 'Phil', '3', 'Alb', '0', 'W']\n",
      "['1998', 'May', '30', '30-May-98', 'SaJ', '2', 'Phil', '3', 'W']\n",
      "['1998', 'May', '31', '31-May-98', 'SaJ', '3', 'Phil', '2', 'L']\n",
      "['1998', 'Jun', '3', '3-Jun-98', 'Phil', '4', 'SaJ', '3', 'W']\n",
      "['1998', 'Jun', '5', '5-Jun-98', 'Phil', '6', 'SaJ', '4', 'W']\n",
      "['1998', 'Jun', '7', '7-Jun-98', 'Phil', '1', 'SaJ', '6', 'L']\n",
      "['1998', 'Jun', '10', '10-Jun-98', 'SaJ', '1', 'Phil', '6', 'W']\n",
      "['1999', 'Apr', '22', '22-Apr-99', 'Cin', '3', 'Phil', '4', 'W']\n",
      "['1999', 'Apr', '24', '24-Apr-99', 'Cin', '2', 'Phil', '5', 'W']\n",
      "['1999', 'Apr', '25', '25-Apr-99', 'Phil', '5', 'Cin', '2', 'W']\n",
      "['1999', 'May', '5', '5-May-99', 'Ken', '6', 'Phil', '3', 'L']\n",
      "['1999', 'May', '7', '7-May-99', 'Ken', '2', 'Phil', '5', 'W']\n",
      "['1999', 'May', '9', '9-May-99', 'Phil', '1', 'Ken', '2', 'L']\n",
      "['1999', 'May', '11', '11-May-99', 'Phil', '4', 'Ken', '3', 'W']\n",
      "['1999', 'May', '13', '13-May-99', 'Phil', '2', 'Ken', '3', 'L']\n",
      "['1999', 'May', '15', '15-May-99', 'Ken', '2', 'Phil', '5', 'W']\n",
      "['1999', 'May', '16', '16-May-99', 'Ken', '3', 'Phil', '9', 'W']\n",
      "['1999', 'May', '20', '20-May-99', 'Rch', '2', 'Phil', '3', 'W']\n",
      "['1999', 'May', '22', '22-May-99', 'Rch', '2', 'Phil', '5', 'W']\n",
      "['1999', 'May', '26', '26-May-99', 'Phil', '0', 'Rch', '3', 'L']\n",
      "['1999', 'May', '28', '28-May-99', 'Phil', '1', 'Rch', '6', 'L']\n",
      "['1999', 'May', '30', '30-May-99', 'Phil', '1', 'Rch', '2', 'L']\n",
      "['1999', 'Jun', '1', '1-Jun-99', 'Rch', '4', 'Phil', '2', 'L']\n",
      "['2000', 'Apr', '11', '11-Apr-00', 'Her', '3', 'Phil', '2', 'L']\n",
      "['2000', 'Apr', '14', '14-Apr-00', 'Phil', '0', 'Her', '2', 'L']\n",
      "['2000', 'Apr', '17', '17-Apr-00', 'Phil', '5', 'Her', '3', 'W']\n",
      "['2000', 'Apr', '19', '19-Apr-00', 'Phil', '6', 'Her', '3', 'W']\n",
      "['2000', 'Apr', '21', '21-Apr-00', 'Her', '4', 'Phil', '0', 'L']\n",
      "['2001', 'Apr', '11', '11-Apr-01', 'Phil', '3', 'Rch', '4', 'L']\n",
      "['2001', 'Apr', '13', '13-Apr-01', 'Rch', '1', 'Phi', '2', 'W']\n",
      "['2001', 'Apr', '14', '14-Apr-01', 'Phil', '2', 'Rch', '0', 'W']\n",
      "['2001', 'Apr', '16', '16-Apr-01', 'Rch', '2', 'Phi', '4', 'W']\n",
      "['2001', 'Apr', '21', '21-Apr-01', 'Phil', '3', 'WBS', '4', 'L']\n",
      "['2001', 'Apr', '23', '23-Apr-01', 'Phil', '2', 'WBS', '4', 'L']\n",
      "['2001', 'Apr', '25', '25-Apr-01', 'WBS', '3', 'Phil', '0', 'L']\n",
      "['2001', 'Apr', '27', '27-Apr-01', 'WBS', '2', 'Phil', '8', 'W']\n",
      "['2001', 'Apr', '28', '28-Apr-01', 'Phil', '3', 'WBS', '2', 'W']\n",
      "['2001', 'May', '1', '1-May-01', 'WBS', '3', 'Phil', '2', 'L']\n",
      "['2001', 'Apr', '9', '9-Apr-01', 'Rch', '2', 'Phil', '4', 'W']\n",
      "['2001', 'Apr', '11', '11-Apr-01', 'Rch', '2', 'Phil', '3', 'W']\n",
      "['2001', 'Apr', '13', '13-Apr-01', 'Phil', '3', 'Syr', '5', 'L']\n",
      "['2001', 'Apr', '17', '17-Apr-01', 'Phil', '1', 'Syr', '4', 'L']\n",
      "['2001', 'Apr', '21', '21-Apr-01', 'Syr', '3', 'Phil', '1', 'L']\n",
      "['2004', 'Apr', '21', '21-Apr-04', 'Nor', '1', 'Phil', '2', 'W']\n",
      "['2004', 'Apr', '23', '23-Apr-04', 'Nor', '2', 'Phil', '1', 'L']\n",
      "['2004', 'Apr', '27', '27-Apr-04', 'Phil', '3', 'Nor', '1', 'W']\n",
      "['2004', 'Apr', '28', '28-Apr-04', 'Phil', '2', 'Nor', '1', 'W']\n",
      "['2004', 'Apr', '30', '30-Apr-04', 'Phil', '1', 'Nor', '2', 'L']\n",
      "['2004', 'May', '1', '1-May-04', 'Nor', '2', 'Phil', '3', 'W']\n",
      "['2004', 'May', '5', '5-May-04', 'W-B', '5', 'Phil', '4', 'L']\n",
      "['2004', 'May', '7', '7-May-04', 'W-B', '2', 'Phil', '3', 'W']\n",
      "['2004', 'May', '8', '8-May-04', 'Phil', '0', 'W-B', '3', 'L']\n",
      "['2004', 'May', '10', '10-May-04', 'Phil', '0', 'W-B', '2', 'L']\n",
      "['2004', 'May', '12', '12-May-04', 'W-B', '2', 'Phil', '4', 'W']\n",
      "['2004', 'May', '14', '14-May-04', 'Phil', '1', 'W-B', '3', 'L']\n",
      "['2005', 'Apr', '22', '22-Apr-05', 'Norf', '1', 'Phantoms', '3', 'W']\n",
      "['2005', 'Apr', '23', '23-Apr-05', 'Norf', '0', 'Phantoms', '3', 'W']\n",
      "['2005', 'Apr', '25', '25-Apr-05', 'Phantoms', '1', 'Norf', '3', 'L']\n",
      "['2005', 'Apr', '27', '27-Apr-05', 'Phantoms', '4', 'Norf', '1', 'W']\n",
      "['2005', 'Apr', '29', '29-Apr-05', 'Phantoms', '1', 'Norf', '4', 'L']\n",
      "['2005', 'May', '1', '1-May-05', 'Norf', '2', 'Phantoms', '4', 'W']\n",
      "['2005', 'May', '4', '4-May-05', 'WBS', '1', 'Phantoms', '4', 'W']\n",
      "['2005', 'May', '6', '6-May-05', 'WBS', '1', 'Phantoms', '4', 'W']\n",
      "['2005', 'May', '7', '7-May-05', 'Phantoms', '1', 'WBS', '3', 'L']\n",
      "['2005', 'May', '10', '10-May-05', 'Phantoms', '5', 'WBS', '3', 'W']\n",
      "['2005', 'May', '13', '13-May-05', 'WBS', '4', 'Phantoms', '7', 'W']\n",
      "['2005', 'May', '20', '20-May-05', 'Prov', '2', 'Phantoms', '4', 'W']\n",
      "['2005', 'May', '21', '21-May-05', 'Prov', '0', 'Phantoms', '3', 'W']\n",
      "['2005', 'May', '23', '23-May-05', 'Phantoms', '1', 'Prov', '2', 'L']\n",
      "['2005', 'May', '25', '25-May-05', 'Phantoms', '2', 'Prov', '1', 'W']\n",
      "['2005', 'May', '27', '27-May-05', 'Phantoms', '4', 'Prov', '6', 'L']\n",
      "['2005', 'Jun', '2', '2-Jun-05', 'Phantoms', '1', 'Chicago Wolves', '0', 'W']\n",
      "['2005', 'Jun', '4', '4-Jun-05', 'Phantoms', '2', 'Chicago Wolves', '1', 'W']\n",
      "['2005', 'Jun', '8', '8-Jun-05', 'Chicago Wolves', '1', 'Phantoms', '2', 'W']\n",
      "['2005', 'Jun', '10', '10-Jun-05', 'Chicago Wolves', '2', 'Phantoms', '5', 'W']\n",
      "['2008', 'Apr', '16', '16-Apr-2008', 'Phantoms', '4', 'Albany', '0', 'L']\n",
      "['2008', 'Apr', '18', '18-Apr-2008', 'Albany', '3', 'Phantoms', '0', 'W']\n",
      "['2008', 'Apr', '20', '20-Apr-2008', 'Albany', '1', 'Phantoms', '4', 'L']\n",
      "['2008', 'Apr', '22', '22-Apr-2008', 'Phantoms', '1', 'Albany', '2', 'W']\n",
      "['2008', 'Apr', '24', '24-Apr-2008', 'Phantoms', '2', 'Albany', '3', 'W']\n",
      "['2008', 'Apr', '26', '26-Apr-2008', 'Albany', '0', 'Phantoms', '1', 'L']\n",
      "['2008', 'Apr', '29', '29-Apr-2008', 'Albany', '2', 'Phantoms', '0', 'W']\n",
      "['2008', 'May', '1', '1-May-2008', 'Phantoms', '2', 'Wilkes-Barre', '5', 'W']\n",
      "['2008', 'May', '3', '3-May-2008', 'Phantoms', '5', 'Wilkes-Barre', '4', 'L']\n",
      "['2008', 'May', '5', '5-May-2008', 'Wilkes-Barre', '2', 'Phantoms', '3', 'L']\n",
      "['2008', 'May', '7', '7-May-2008', 'Wilkes-Barre', '4', 'Phantoms', '8', 'L']\n",
      "['2008', 'May', '9', '9-May-2008', 'Phantoms', '3', 'Wilkes-Barre', '1', 'L']\n",
      "['2009', 'Apr', '16', '16-Apr-09', 'Phantoms', '2', 'Hershey', '4', 'L']\n",
      "['2009', 'Apr', '18', '18-Apr-09', 'Phantoms', '2', 'Hershey', '6', 'L']\n",
      "['2009', 'Apr', '22', '22-Apr-09', 'Hershey', '2', 'Phantoms', '3', 'L']\n",
      "['2009', 'Apr', '24', '24-Apr-09', 'Hershey', '0', 'Phantoms', '1', 'L']\n",
      "['2017', 'Apr', '21', '21-Apr-17', 'Hershey', '1', 'Phantoms', '0', 'L']\n",
      "['2017', 'Apr', '22', '22-Apr-17', 'Hershey', '5', 'Phantoms', '4', 'L']\n",
      "['2017', 'Apr', '26', '26-Apr-17', 'Phantoms', '2', 'Hershey', '1', 'W']\n",
      "['2017', 'Apr', '29', '29-Apr-17', 'Phantoms', '4', 'Hershey', '1', 'W']\n",
      "['2017', 'Apr', '30', '30-Apr-17', 'Hershey', '3', 'Phantoms', '2', 'L']\n",
      "['2018', 'Apr', '20', '20-Apr-18', 'Phantoms', '3', 'Providence', '2', 'W']\n",
      "['2018', 'Apr', '21', '21-Apr-18', 'Phantoms', '3', 'Providence', '5', 'L']\n",
      "['2018', 'Apr', '27', '27-Apr-18', 'Providence', '1', 'Phantoms', '4', 'W']\n",
      "['2018', 'Apr', '28', '28-Apr-18', 'Providence', '3', 'Phantoms', '4', 'W']\n",
      "['2018', 'May', '4', '4-May-18', 'Charlotte', '1', 'Phantoms', '2', 'W']\n",
      "['2018', 'May', '5', '5-May-18', 'Charlotte', '6', 'Phantoms', '0', 'L']\n",
      "['2018', 'May', '6', '6-May-18', 'Phantoms', '5', 'Charlotte', '1', 'W']\n",
      "['2018', 'May', '9', '9-May-18', 'Phantoms', '2', 'Charlotte', '1', 'W']\n",
      "['2018', 'May', '12', '12-May-18', 'Phantoms', '5', 'Charlotte', '1', 'W']\n",
      "['2018', 'May', '19', '19-May-18', 'Phantoms', '3', 'Toronto', '4', 'L']\n",
      "['2018', 'May', '20', '20-May-18', 'Phantoms', '2', 'Toronto', '3', 'L']\n",
      "['2018', 'May', '23', '23-May-18', 'Toronto', '5', 'Phantoms', '0', 'L']\n",
      "['2018', 'May', '25', '25-May-18', 'Toronto', '5', 'Phantoms', '1', 'L']\n"
     ]
    }
   ],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "import re\n",
    "\n",
    "sourceUrl = 'http://www.flyershistory.com/cgi-bin/ml-poffs.cgi'\n",
    "dataSet = list()\n",
    "keys = ['year','month','day','game_date','team1', 'team1_score', 'team2', 'team2_score', 'game_status']\n",
    "\n",
    "def read_url(url):\n",
    "    \"\"\"Read given Url , Returns pyquery object for page content\"\"\"\n",
    "    pageSource = pq(url)\n",
    "    return pq(pageSource)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    page = read_url(sourceUrl)\n",
    "\n",
    "    tableRows = page.find(\"h1:contains('AHL Playoff Results') + table tr\")\n",
    "    print(\"\\nTotal rows found :\", tableRows.__len__())\n",
    "\n",
    "    for tr in tableRows.items():\n",
    "        team1 = tr.find('td').eq(1).text()\n",
    "        if team1 != '':\n",
    "            game_date = tr.find('td').eq(0).text()\n",
    "            dates = re.search(r'(.*)-(.*)-(.*)',game_date)\n",
    "\n",
    "            team1_score = tr.find('td').eq(2).text()\n",
    "            team2 = tr.find('td').eq(4).text()\n",
    "            team2_score = tr.find('td').eq(5).text()\n",
    "\n",
    "            #check Game Status should be either 'W' or 'L'\n",
    "            game_status = tr.find('td').eq(6).text()\n",
    "            if not re.match(r'[WL]',game_status):\n",
    "                game_status = tr.find('td').eq(7).text()\n",
    "\n",
    "            #breaking down date in year,month and day\n",
    "            year = dates.group(3)\n",
    "            month = dates.group(2)\n",
    "            day = dates.group(1)\n",
    "            if len(year)==2 and int(year)>=68:\n",
    "                year = '19'+year\n",
    "            elif len(year)==2 and int(year) <68:\n",
    "                year = '20'+year\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            #appending individual data list to the dataSet\n",
    "            dataSet.append([year,month,day,game_date,team1,team1_score,team2,team2_score,game_status])\n",
    "\n",
    "    print(\"\\nTotal Game Status, found :\", len(dataSet))\n",
    "    for data in dataSet:\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 4 - Coletando URLs de um sitemap.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso 1 - Usando HTML parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Children Length:  135\n",
      "First Children:  <url>\n",
      "<loc>https://webscraping.com/blog/What-is-web-scraping/</loc>\n",
      "<lastmod>2009-12-20T00:00:00+00:00</lastmod>\n",
      "</url>\n",
      "\n",
      "Inner Child/First Children:  <loc>https://webscraping.com/blog/What-is-web-scraping/</loc>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "if __name__ == '__main__':\n",
    "    # reading file\n",
    "    xmlFile = open('sitemap.xml', 'r').read().encode()\n",
    "    urlHTML = pq(xmlFile, parser='html')\n",
    "print(\"Children Length: \",urlHTML.children().__len__())\n",
    "print(\"First Children: \",urlHTML.children().eq(0))\n",
    "print(\"Inner Child/First Children: \",urlHTML.children().children().eq(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do DataSet: 80\n",
      "https://webscraping.com/blog/What-is-web-scraping/\n",
      "https://webscraping.com/blog/Typical-web-scraping-job/\n",
      "https://webscraping.com/blog/Parsing-HTML-with-Python/\n",
      "https://webscraping.com/blog/How-to-use-XPaths-robustly/\n",
      "https://webscraping.com/blog/Web-scraping-with-regular-expressions/\n",
      "https://webscraping.com/blog/The-SiteScraper-module/\n",
      "https://webscraping.com/blog/Why-Python/\n",
      "https://webscraping.com/blog/How-to-protect-your-data/\n",
      "https://webscraping.com/blog/How-to-crawl-websites-without-being-blocked/\n",
      "https://webscraping.com/blog/Scraping-JavaScript-based-web-pages-with-Chickenfoot/\n",
      "https://webscraping.com/blog/Scraping-JavaScript-webpages-with-webkit/\n",
      "https://webscraping.com/blog/I-love-AJAX/\n",
      "https://webscraping.com/blog/Scraping-Flash-based-websites/\n",
      "https://webscraping.com/blog/Scraping-dynamic-data/\n",
      "https://webscraping.com/blog/Why-Google-App-Engine/\n",
      "https://webscraping.com/blog/Why-web2py/\n",
      "https://webscraping.com/blog/Open-sourced-web-scraping-code/\n",
      "https://webscraping.com/blog/Fixed-fee-or-hourly/\n",
      "https://webscraping.com/blog/Caching-crawled-webpages/\n",
      "https://webscraping.com/blog/Best-website-for-freelancers/\n",
      "https://webscraping.com/blog/Why-reinvent-the-wheel/\n",
      "https://webscraping.com/blog/Client-Feedback/\n",
      "https://webscraping.com/blog/Image-efficiencies/\n",
      "https://webscraping.com/blog/Extracting-article-summaries/\n",
      "https://webscraping.com/blog/Increase-your-Google-App-Engine-quotas-for-free/\n",
      "https://webscraping.com/blog/New-scraping-quote-tool/\n",
      "https://webscraping.com/blog/Automating-CAPTCHAs/\n",
      "https://webscraping.com/blog/Google-Storage/\n",
      "https://webscraping.com/blog/Crawling-with-threads/\n",
      "https://webscraping.com/blog/Using-Google-Cache-to-crawl-a-website/\n",
      "https://webscraping.com/blog/Using-Google-Translate-to-crawl-a-website/\n",
      "https://webscraping.com/blog/Google-App-Engine-limitations/\n",
      "https://webscraping.com/blog/Parsing-Flash-with-Swiffy/\n",
      "https://webscraping.com/blog/Taking-advantage-of-mobile-interfaces/\n",
      "https://webscraping.com/blog/User-agents/\n",
      "https://webscraping.com/blog/Can-you-extract-data-from-this-website/\n",
      "https://webscraping.com/blog/Webpage-screenshots-with-webkit/\n",
      "https://webscraping.com/blog/Free-service-to-extract-article-from-webpage/\n",
      "https://webscraping.com/blog/How-to-automatically-find-contact-details/\n",
      "https://webscraping.com/blog/How-to-use-proxies/\n",
      "https://webscraping.com/blog/How-to-teach-yourself-web-scraping/\n",
      "https://webscraping.com/blog/Scraping-multiple-JavaScript-webpages-with-webkit/\n",
      "https://webscraping.com/blog/Threading-with-webkit/\n",
      "https://webscraping.com/blog/Automatic-web-scraping/\n",
      "https://webscraping.com/blog/How-to-make-python-faster/\n",
      "https://webscraping.com/blog/Caching-data-efficiently/\n",
      "https://webscraping.com/blog/Automating-webkit/\n",
      "https://webscraping.com/blog/Is-Web-Scraping-Legal/\n",
      "https://webscraping.com/blog/Useful-business-directories/\n",
      "https://webscraping.com/blog/Solving-CAPTCHA/\n",
      "https://webscraping.com/blog/Converting-UK-Easting-Northing-coordinates/\n",
      "https://webscraping.com/blog/What-is-CSV/\n",
      "https://webscraping.com/blog/How-to-find-what-technology-a-website-uses/\n",
      "https://webscraping.com/blog/Using-the-internet-archive-to-crawl-a-website/\n",
      "https://webscraping.com/blog/Asynchronous-support-in-Python/\n",
      "https://webscraping.com/blog/Importing-CSV-into-MySQL/\n",
      "https://webscraping.com/blog/Web-Scraping-Interface/\n",
      "https://webscraping.com/blog/Generate-website-screenshot-history/\n",
      "https://webscraping.com/blog/Startup/\n",
      "https://webscraping.com/blog/Services/\n",
      "https://webscraping.com/blog/Reverse-Geocode/\n",
      "https://webscraping.com/blog/Loading-Cookies-from-the-Browser/\n",
      "https://webscraping.com/blog/Web-Scrapping/\n",
      "https://webscraping.com/blog/Android-Apps-Update/\n",
      "https://webscraping.com/blog/Bitcoin/\n",
      "https://webscraping.com/blog/Phone-Calls/\n",
      "https://webscraping.com/blog/Luminati/\n",
      "https://webscraping.com/blog/How-to-scrape-Android-Apps/\n",
      "https://webscraping.com/blog/Heading-to-Oxford/\n",
      "https://webscraping.com/blog/Web-Scraping-book-published/\n",
      "https://webscraping.com/blog/How-to-automate-Android-Apps/\n",
      "https://webscraping.com/blog/how-to-intercept-secure-network-traffic-from-android/\n",
      "https://webscraping.com/blog/\n",
      "https://webscraping.com/blog/2/\n",
      "https://webscraping.com/blog/3/\n",
      "https://webscraping.com/blog/4/\n",
      "https://webscraping.com/blog/5/\n",
      "https://webscraping.com/blog/6/\n",
      "https://webscraping.com/blog/7/\n",
      "https://webscraping.com/blog/8/\n"
     ]
    }
   ],
   "source": [
    "dataSet = list()\n",
    "for url in urlHTML.children().find('loc:contains(\"blog\")').items():\n",
    "    dataSet.append(url.text())\n",
    "\n",
    "print(f\"Tamanho do DataSet: {len(dataSet)}\")\n",
    "for data in dataSet:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso 2 - Usando o XML parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Children Length:  135\n",
      "First Children:  <url xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\n",
      "<loc>https://webscraping.com/blog/What-is-web-scraping/</loc>\n",
      "<lastmod>2009-12-20T00:00:00+00:00</lastmod>\n",
      "</url>\n",
      "\n",
      "Inner Child/First Children:  <loc xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">https://webscraping.com/blog/What-is-web-scraping/</loc>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "if __name__ == '__main__':\n",
    "    # reading file\n",
    "    xmlFile = open('sitemap.xml', 'r').read().encode()\n",
    "    urlXML = pq(xmlFile, parser='xml')\n",
    "print(\"Children Length: \",urlHTML.children().__len__())\n",
    "print(\"First Children: \",urlHTML.children().eq(0))\n",
    "print(\"Inner Child/First Children: \",urlHTML.children().children().eq(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do DataSet: 0\n"
     ]
    }
   ],
   "source": [
    "dataSet = list()\n",
    "for url in urlHTML.children().find('loc:contains(\"blog\")').items():\n",
    "    dataSet.append(url.text())\n",
    "\n",
    "print(f\"Tamanho do DataSet: {len(dataSet)}\")\n",
    "for data in dataSet:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não foi recebido nenhum output do dataSet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<loc xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">https://webscraping.com/blog/What-is-web-scraping/</loc>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vamos observar o seguinte código.\n",
    "for url in urlXML.children().children().items():\n",
    "    print(url)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `remove_namespace()` pode ser usada em um objeto PyQuery e processada para sua saída final, conforme mostrado no código a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do DataSet: 80\n",
      "https://webscraping.com/blog/What-is-web-scraping/\n",
      "https://webscraping.com/blog/Typical-web-scraping-job/\n",
      "https://webscraping.com/blog/Parsing-HTML-with-Python/\n",
      "https://webscraping.com/blog/How-to-use-XPaths-robustly/\n",
      "https://webscraping.com/blog/Web-scraping-with-regular-expressions/\n",
      "https://webscraping.com/blog/The-SiteScraper-module/\n",
      "https://webscraping.com/blog/Why-Python/\n",
      "https://webscraping.com/blog/How-to-protect-your-data/\n",
      "https://webscraping.com/blog/How-to-crawl-websites-without-being-blocked/\n",
      "https://webscraping.com/blog/Scraping-JavaScript-based-web-pages-with-Chickenfoot/\n",
      "https://webscraping.com/blog/Scraping-JavaScript-webpages-with-webkit/\n",
      "https://webscraping.com/blog/I-love-AJAX/\n",
      "https://webscraping.com/blog/Scraping-Flash-based-websites/\n",
      "https://webscraping.com/blog/Scraping-dynamic-data/\n",
      "https://webscraping.com/blog/Why-Google-App-Engine/\n",
      "https://webscraping.com/blog/Why-web2py/\n",
      "https://webscraping.com/blog/Open-sourced-web-scraping-code/\n",
      "https://webscraping.com/blog/Fixed-fee-or-hourly/\n",
      "https://webscraping.com/blog/Caching-crawled-webpages/\n",
      "https://webscraping.com/blog/Best-website-for-freelancers/\n",
      "https://webscraping.com/blog/Why-reinvent-the-wheel/\n",
      "https://webscraping.com/blog/Client-Feedback/\n",
      "https://webscraping.com/blog/Image-efficiencies/\n",
      "https://webscraping.com/blog/Extracting-article-summaries/\n",
      "https://webscraping.com/blog/Increase-your-Google-App-Engine-quotas-for-free/\n",
      "https://webscraping.com/blog/New-scraping-quote-tool/\n",
      "https://webscraping.com/blog/Automating-CAPTCHAs/\n",
      "https://webscraping.com/blog/Google-Storage/\n",
      "https://webscraping.com/blog/Crawling-with-threads/\n",
      "https://webscraping.com/blog/Using-Google-Cache-to-crawl-a-website/\n",
      "https://webscraping.com/blog/Using-Google-Translate-to-crawl-a-website/\n",
      "https://webscraping.com/blog/Google-App-Engine-limitations/\n",
      "https://webscraping.com/blog/Parsing-Flash-with-Swiffy/\n",
      "https://webscraping.com/blog/Taking-advantage-of-mobile-interfaces/\n",
      "https://webscraping.com/blog/User-agents/\n",
      "https://webscraping.com/blog/Can-you-extract-data-from-this-website/\n",
      "https://webscraping.com/blog/Webpage-screenshots-with-webkit/\n",
      "https://webscraping.com/blog/Free-service-to-extract-article-from-webpage/\n",
      "https://webscraping.com/blog/How-to-automatically-find-contact-details/\n",
      "https://webscraping.com/blog/How-to-use-proxies/\n",
      "https://webscraping.com/blog/How-to-teach-yourself-web-scraping/\n",
      "https://webscraping.com/blog/Scraping-multiple-JavaScript-webpages-with-webkit/\n",
      "https://webscraping.com/blog/Threading-with-webkit/\n",
      "https://webscraping.com/blog/Automatic-web-scraping/\n",
      "https://webscraping.com/blog/How-to-make-python-faster/\n",
      "https://webscraping.com/blog/Caching-data-efficiently/\n",
      "https://webscraping.com/blog/Automating-webkit/\n",
      "https://webscraping.com/blog/Is-Web-Scraping-Legal/\n",
      "https://webscraping.com/blog/Useful-business-directories/\n",
      "https://webscraping.com/blog/Solving-CAPTCHA/\n",
      "https://webscraping.com/blog/Converting-UK-Easting-Northing-coordinates/\n",
      "https://webscraping.com/blog/What-is-CSV/\n",
      "https://webscraping.com/blog/How-to-find-what-technology-a-website-uses/\n",
      "https://webscraping.com/blog/Using-the-internet-archive-to-crawl-a-website/\n",
      "https://webscraping.com/blog/Asynchronous-support-in-Python/\n",
      "https://webscraping.com/blog/Importing-CSV-into-MySQL/\n",
      "https://webscraping.com/blog/Web-Scraping-Interface/\n",
      "https://webscraping.com/blog/Generate-website-screenshot-history/\n",
      "https://webscraping.com/blog/Startup/\n",
      "https://webscraping.com/blog/Services/\n",
      "https://webscraping.com/blog/Reverse-Geocode/\n",
      "https://webscraping.com/blog/Loading-Cookies-from-the-Browser/\n",
      "https://webscraping.com/blog/Web-Scrapping/\n",
      "https://webscraping.com/blog/Android-Apps-Update/\n",
      "https://webscraping.com/blog/Bitcoin/\n",
      "https://webscraping.com/blog/Phone-Calls/\n",
      "https://webscraping.com/blog/Luminati/\n",
      "https://webscraping.com/blog/How-to-scrape-Android-Apps/\n",
      "https://webscraping.com/blog/Heading-to-Oxford/\n",
      "https://webscraping.com/blog/Web-Scraping-book-published/\n",
      "https://webscraping.com/blog/How-to-automate-Android-Apps/\n",
      "https://webscraping.com/blog/how-to-intercept-secure-network-traffic-from-android/\n",
      "https://webscraping.com/blog/\n",
      "https://webscraping.com/blog/2/\n",
      "https://webscraping.com/blog/3/\n",
      "https://webscraping.com/blog/4/\n",
      "https://webscraping.com/blog/5/\n",
      "https://webscraping.com/blog/6/\n",
      "https://webscraping.com/blog/7/\n",
      "https://webscraping.com/blog/8/\n"
     ]
    }
   ],
   "source": [
    "for url in urlHTML.remove_namespaces().children().find('loc:contains(\"blog\")').items(): # Agora vai funcionar!!\n",
    "    dataSet.append(url.text())\n",
    "\n",
    "print(f\"Tamanho do DataSet: {len(dataSet)}\")\n",
    "for data in dataSet:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of blogXML:  210\n",
      "Length of dataSet:  80\n",
      "Blog Urls:  ['https://webscraping.com/blog/What-is-web-scraping/', 'https://webscraping.com/blog/Typical-web-scraping-job/', 'https://webscraping.com/blog/Parsing-HTML-with-Python/', 'https://webscraping.com/blog/How-to-use-XPaths-robustly/', 'https://webscraping.com/blog/Web-scraping-with-regular-expressions/', 'https://webscraping.com/blog/The-SiteScraper-module/', 'https://webscraping.com/blog/Why-Python/', 'https://webscraping.com/blog/How-to-protect-your-data/', 'https://webscraping.com/blog/How-to-crawl-websites-without-being-blocked/', 'https://webscraping.com/blog/Scraping-JavaScript-based-web-pages-with-Chickenfoot/', 'https://webscraping.com/blog/Scraping-JavaScript-webpages-with-webkit/', 'https://webscraping.com/blog/I-love-AJAX/', 'https://webscraping.com/blog/Scraping-Flash-based-websites/', 'https://webscraping.com/blog/Scraping-dynamic-data/', 'https://webscraping.com/blog/Why-Google-App-Engine/', 'https://webscraping.com/blog/Why-web2py/', 'https://webscraping.com/blog/Open-sourced-web-scraping-code/', 'https://webscraping.com/blog/Fixed-fee-or-hourly/', 'https://webscraping.com/blog/Caching-crawled-webpages/', 'https://webscraping.com/blog/Best-website-for-freelancers/', 'https://webscraping.com/blog/Why-reinvent-the-wheel/', 'https://webscraping.com/blog/Client-Feedback/', 'https://webscraping.com/blog/Image-efficiencies/', 'https://webscraping.com/blog/Extracting-article-summaries/', 'https://webscraping.com/blog/Increase-your-Google-App-Engine-quotas-for-free/', 'https://webscraping.com/blog/New-scraping-quote-tool/', 'https://webscraping.com/blog/Automating-CAPTCHAs/', 'https://webscraping.com/blog/Google-Storage/', 'https://webscraping.com/blog/Crawling-with-threads/', 'https://webscraping.com/blog/Using-Google-Cache-to-crawl-a-website/', 'https://webscraping.com/blog/Using-Google-Translate-to-crawl-a-website/', 'https://webscraping.com/blog/Google-App-Engine-limitations/', 'https://webscraping.com/blog/Parsing-Flash-with-Swiffy/', 'https://webscraping.com/blog/Taking-advantage-of-mobile-interfaces/', 'https://webscraping.com/blog/User-agents/', 'https://webscraping.com/blog/Can-you-extract-data-from-this-website/', 'https://webscraping.com/blog/Webpage-screenshots-with-webkit/', 'https://webscraping.com/blog/Free-service-to-extract-article-from-webpage/', 'https://webscraping.com/blog/How-to-automatically-find-contact-details/', 'https://webscraping.com/blog/How-to-use-proxies/', 'https://webscraping.com/blog/How-to-teach-yourself-web-scraping/', 'https://webscraping.com/blog/Scraping-multiple-JavaScript-webpages-with-webkit/', 'https://webscraping.com/blog/Threading-with-webkit/', 'https://webscraping.com/blog/Automatic-web-scraping/', 'https://webscraping.com/blog/How-to-make-python-faster/', 'https://webscraping.com/blog/Caching-data-efficiently/', 'https://webscraping.com/blog/Automating-webkit/', 'https://webscraping.com/blog/Is-Web-Scraping-Legal/', 'https://webscraping.com/blog/Useful-business-directories/', 'https://webscraping.com/blog/Solving-CAPTCHA/', 'https://webscraping.com/blog/Converting-UK-Easting-Northing-coordinates/', 'https://webscraping.com/blog/What-is-CSV/', 'https://webscraping.com/blog/How-to-find-what-technology-a-website-uses/', 'https://webscraping.com/blog/Using-the-internet-archive-to-crawl-a-website/', 'https://webscraping.com/blog/Asynchronous-support-in-Python/', 'https://webscraping.com/blog/Importing-CSV-into-MySQL/', 'https://webscraping.com/blog/Web-Scraping-Interface/', 'https://webscraping.com/blog/Generate-website-screenshot-history/', 'https://webscraping.com/blog/Startup/', 'https://webscraping.com/blog/Services/', 'https://webscraping.com/blog/Reverse-Geocode/', 'https://webscraping.com/blog/Loading-Cookies-from-the-Browser/', 'https://webscraping.com/blog/Web-Scrapping/', 'https://webscraping.com/blog/Android-Apps-Update/', 'https://webscraping.com/blog/Bitcoin/', 'https://webscraping.com/blog/Phone-Calls/', 'https://webscraping.com/blog/Luminati/', 'https://webscraping.com/blog/How-to-scrape-Android-Apps/', 'https://webscraping.com/blog/Heading-to-Oxford/', 'https://webscraping.com/blog/Web-Scraping-book-published/', 'https://webscraping.com/blog/How-to-automate-Android-Apps/', 'https://webscraping.com/blog/how-to-intercept-secure-network-traffic-from-android/', 'https://webscraping.com/blog/', 'https://webscraping.com/blog/2/', 'https://webscraping.com/blog/3/', 'https://webscraping.com/blog/4/', 'https://webscraping.com/blog/5/', 'https://webscraping.com/blog/6/', 'https://webscraping.com/blog/7/', 'https://webscraping.com/blog/8/']\n"
     ]
    }
   ],
   "source": [
    "blogXML = re.split(r'\\s',urlXML .children().text())\n",
    "print(\"Length of blogXML: \",len(blogXML))\n",
    "#filter(), filters URLs from blogXML that matches string 'blog'\n",
    "dataSet= list(filter(lambda blogXML:re.findall(r'blog',blogXML),blogXML))\n",
    "print(\"Length of dataSet: \",len(dataSet))\n",
    "print(\"Blog Urls: \",dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
